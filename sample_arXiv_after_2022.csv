Name,Text,Pre2022
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Recent instruction fine-tuned models can solve multiple NLP tasks when prompted to do so, with machine translation (MT) being a prominent use case. However, current research often focuses on standard performance benchmarks, leaving compelling fairness and ethical considerations behind. In MT, this might lead to misgendered translations, resulting, among other harms, in the perpetuation of stereotypes and prejudices. In this work, we address this gap by investigating whether and to what extent such models exhibit gender bias in machine translation and how we can mitigate it.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Concretely, we compute established gender bias metrics on the WinoMT corpus from English to German and Spanish. We discover that IFT models default to male-inflected translations, even disregarding female occupational stereotypes. Next, using interpretability methods, we unveil that models systematically overlook the pronoun indicating the gender of a target occupation in misgendered translations. Finally, based on this finding, we propose an easy-to-implement and effective bias mitigation solution based on few shot learning that leads to significantly fairer translations.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Instruction fine-tuned (IFT) models, such as FlanT5 (Chung et al., 2022) and mT0 (Muennighoff et al., 2023a), are trained on large corpora of machine learning tasks verbalized in natural language and learned through standard language modeling. The large and diverse mixture of training tasks has led to unmatched transfer performance – if prompted properly, models are able to virtually solve any standard NLP task, including sentiment analysis, natural language inference, question answering, and more (Sanh et al., 2022).",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"However, most efforts on their evaluation have focused on standard benchmarks only, with a prominent focus on testing zero-shot abilities (Chung et al., 2022) and cross-lingual generalization (Muennighoff et al., 2023b), and have thus largely ignored the models’ social impact (Hovy and Spruit, 2016). This lacuna is extremely surprising as (a) IFT models are based on pretrained language models, which are widely known to encode societal biases and unfair stereotypes (Nadeem et al., 2021; Nozza et al., 2021, inter alia); and (b) exposing models to many fine-tuning sources can exacerbate biased behaviors as stereotypical demonstrations add up (Srivastava et al., 2022).",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"As a result, we expect instruction-tuned models to encode societal biases and unfair stereotypes, possibly even beyond the extent of their base models. Still, few efforts have been spent on bias evaluation and mitigation for these models so far (a notable exception being provided by Akyürek et al. (2022)), putting their societal beneficial use at risk. In this work, we address this research gap by studying occupational gender bias in zero- and few-shot setups in one of the, arguably, most prominent NLP applications to date, machine translation (MT).",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Based on attribution interpretability, we find that models systematically ignore the pronoun (and thus, the conveyed gender information) when producing misgendered translations. In contrast, correctly translated professions relate to higher contributions of the pronoun in the choices taken. (3) Based on our insights, we propose a novel and easy-to-use bias mitigation method – informed by interpretability scores! The differences in the attribution scores lead us to hypothesize that models that are used in a fewshot setup would benefit from provided translations mostly, if exactly in those examples they would normally overlook the pronoun.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We hence propose a few-shot learning-based debiasing approach, in which we use interpretability scores to select the incontext exemplars. Figure 1 shows an example of the resulting approach. The solution is simple-yet-effective, leading to significantly fairer translations with as few as four human-translated exemplars. Overall, our findings prove interpretability as a valuable tool for studying and mitigating bias in language models, both as a diagnostic tool and a signal driving bias mitigation approaches. We release code and data artifacts hoping to foster future research in this direction.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Our contributions are three-fold: (1) we provide one the few studies on bias in instruction-tuned models to-date. Focusing on the example of MT and gender bias, we show that despite getting better at zero-shot translation, such models default to male-inflected translations, even in the presence of overt female pronouns and disregarding female occupational stereotypes. (2) To our knowledge, we are among the first to acknowledge the potential of interpretability methods to study IFT language models and why they produce biased predictions.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"To this end, we use the established WinoMT benchmark (Stanovsky et al., 2019) and study the translation from English to Spanish and German, two morphologically diverse languages that both require inflecting multiple syntactic items. We experiment with Flan-T5 and mT0, two state-of-the-art IFT models, controlling for several factors such as the prompt template, model size, and decoding strategy. Importantly, we make use of established interpretability tools to shed light on when and how such models use lexical clues when picking the right (or wrong) gender inflection for a target profession. We then use those insights for informing an easy-to-use and effective bias mitigation approach.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"The primary use case for instruction-tuned models is to tackle standard NLP tasks by formulating a specific request in the input prompt. Here, we experiment with MT, triggered by a specific phrasing such as “Translate this into Spanish.” In particular, we set to study whether such models exhibit gender bias concerning occupations. While doing so, we apply established interpretability metrics to explain why the model preferred specific gender inflections. Later (§4), we propose a novel debiasing approach based on few-shot learning informed by the interpretability findings.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We expect LMs to inflect gender in occupation words according to overt contextual and lexical clues. Instead, a biased model is one, which relies on stereotypical gender-role associations. Both open source and commercial MT systems have been shown to rely on these associations, with a marked tendency to associate women with less prestigious roles (e.g., Stanovsky et al., 2019; Saunders and Byrne, 2020; Chung et al., 2022, inter alia). Echoing Blodgett et al. (2020), such systems risk representational harms, as they portray women in a less favorable light than men.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We base our experiments on WinoMT (Stanovsky et al., 2019), a well-known benchmark for evaluating gender bias in MT. The collection is based on templates. Each instance mentions two professions and a pronoun coreferent to one of them (see Figure 1 for an example). When translating from English, a notional gender language, to Spanish or German, two grammatical gender languages, the pronoun dictates the coreferent inflection because of syntactic agreement.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"For example, the sentence in Figure 1 with “she” as the leading pronoun should translate to “La mecánica” (eng: the female mechanic). The task is challenging as many of the occupations found in WinoMT have stereotypical gender-role associations in society (e.g., nurse to women, developer to men). Indeed, the WinoMT corpus distinguishes between stereotypical and anti-stereotypical templates, which permits us to derive more insights.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"For every translated instance, we compute and collect word attribution interpretability scores from target to source tokens. Word attribution scores (i.e., saliency scores), measure each input token’s contribution to the choice of any translation token. We compute word attributions as follows: first, we extract raw attribution scores using Integrated Gradients (IG; Sundararajan et al., 2017), a commonly used feature attribution algorithm.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Word attribution scores provide a clear, measurable quantity to inspect and debug machine translation models. We, therefore, study such scores and relate recurring patterns to misgendered translations. We extracted several word attribution scores. First, we observe the “alignment” importance between translations aprof,prof , the importance of the English profession word for the target profession (mechanic and mecánico in Figure 1). Then, we also report a control attribution score actrl,prof as the importance of the first source token (“The”) toward the target profession.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We tested both Fast Align (Dyer et al., 2013) and a custom dictionary matching approach and proceeded with the latter because of better quality. In particular, we prompted GPT-3.5 (Ouyang et al., 2022, gpt-3.5-turbo, accessed in early June 2023) to translate all the target professions in WinoMT into the masculine and feminine inflected forms in Spanish and German. After checking and fixing any grammatical errors, we perform hard string matching of the MT output against the word translations.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We study variants of two recently introduced IFT models. Flan-T5 (Chung et al., 2022) is a sequence-to-sequence language model based on the T5 architecture (Raffel et al., 2020). The model has been pre-trained with standard language modeling objectives and subsequently fine-tuned on the FLAN collection (Longpre et al., 2023), counting more than 1,800 NLP tasks in over 60 languages. We test the 80M (Small), 250M (Base), 780M (Large), 3B (XL), and 11B (XXL) model sizes. mT0 (Muennighoff et al., 2023b) is a mT5 model (Xue et al., 2020) fine-tuned on xP3, covering 13 tasks across 46 languages with English prompts. We test the 300M (Small), 580M (Base), 1.2B (Large), 3.7B (XL), and 13B (XXL) model sizes. Both model types have been fine-tuned verbalizing NLP tasks into a text-to-text format and using standard encoder-decoder language modeling loss.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We consider two standard prompt templates and five decoding strategies to account for possible variations with instruction-tuned models. See Appendix C.2 for details. In order to assess the translation quality and select the best instruction-tuned model configuration, we perform an extensive evaluation within a benchmark evaluation framework. We use the state-of-the-art Europarl corpus (Koehn, 2005) to evaluate zero-shot translation quality. We use the benchmark evaluation metrics COMET (reference-based -22 (Rei et al., 2022) and reference-free -20 (Rei et al., 2020)) and BERTScore (Zhang et al., 2019). We also include BLEU (-2 and -4) (Papineni et al., 2002) for comparison with prior work.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Table 1 reports the zero-shot performance of Flan-T5 and mT0 compared to supervised baseline Marian NMT models (Junczys-Dowmunt et al., 2018). Flan-T5 and mT0 slightly underperform supervised baselines. However, they show competitive zero-shot performance as measured by COMET-22 and BERTScore. COMET-20 quality estimation metric show less encouraging results, especially for Flan-T5 (see Table 9 for a full breakdown). Overall, these results suggest that zero-shot translation with instruction-tuned models is almost as valid as specialized supervised models, further motivating their adoption in real use cases.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Table 2 reports the results on WinoMT gender bias metrics. We report several interesting findings. Generally, Flan-T5 is competitive. For both languages, it significantly outperforms mT0 in terms of accuracy and bias evaluation. Moreover, considering commercial systems reported in Stanovsky et al. (2019), GPT-3.5 (Ouyang et al., 2022, gpt-3.5-turbo, accessed in early June 2023), and our supervised baseline, Flan-T5 achieves the best accuracy.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Word attribution scores give us additional insights into the model’s biased behavior. Table 3 shows the average word attribution scores introduced in Section 2.2 grouped by model, language, gender, and stereotypical and anti-stereotypical cases. The table also provides disaggregated accuracy for better understanding. Using our dictionary-based string matching, we found the target profession (i.e., inflected in either of the two forms) in 64% (En-Es) and 39% (En-De) for Flan-T5 and 70% and 49% for mT0.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Male cases are always associated with the highest accuracy, with a difference of 21% and 62% between male and female cases for Flan-T5 and mT0, respectively. Moreover, stereotypical male cases hold the highest performance across all groups. This finding highlights (1) a strong tendency to default to masculine forms and (2) that male stereotypical cases are easier to translate on average.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"More insightful findings can be derived by the word attribution score apron,prof , i.e., the source pronoun importance for translating the gendered profession. Intuitively, source pronoun should be the model’s primary source of information for selecting the correct gender inflection. If we observe low values for this score, we can assume the model has ignored the pronoun for translating. This pattern is especially true for stereotypical male cases: despite their high accuracy, apron,prof scores are low. We observed an opposite trend for stereotypical female cases, where apron,prof scores are the highest, but accuracy is low. Interestingly, apron,prof is highly asymmetrical between female and male cases.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"In six out of eight (model, language, and stereotype) groups, apron,prof is higher for females than males. Regarding stereotypical vs. anti-stereotypical occupations, apron,prof is higher for the latter on three out of four model-language pairs. This statistic supports the intuition that anti-stereotypical cases are where the model is most challenged, particularly for female professions, which consistently have the lowest accuracy. These findings, taken together, reveal a concerning bias in the way professions are portrayed in the models. Even after making an extra effort to consider pronouns, professions are frequently translated into their male inflection, even when they would be stereotypically associated with the female gender.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Interestingly, models attend to the source pronoun sensibly less when wrongly translating female referents (-14% in both anti-stereotypical and stereotypical cases), but the same is not valid for male cases. All these results support the use of ad-hoc interpretability methods for discovering word attribution scores associations with desirable (or undesirable) behavior, thereby serving as proxies for subsequent interventions.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Taking stock of the findings in Section 3, we know that models overtly ignore gender-marking pronouns but also that interpretability scores provide us with a reliable proxy for the phenomenon. Therefore, we hypothesize we can reduce the model’s errors and, in turn, its translation bias by “showing” examples where it would typically overlook the pronoun, each accompanied by a correct translation. Building on recent evidence that large models can solve tasks via in-context learning (Brown et al., 2020b), we implement this intuition via few-shot prompting. Crucially, we use interpretability scores to select in-context exemplars.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We proceed as follows. First, we extract examples with lowest apron,prof importance score, i.e., instances where the model relied the least on the gender-marking pronoun to inflect the profession word. Then, we sample N exemplars from this initial pool and let them be translated by humans. Finally, we use these exemplars as few-shot seeds, simply prepending them to the prompt. Figure 1 shows as end-to-end example of the process.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We experiment with N=4, sampling directly from WinoMT, and stratifying on stereotypical/antistereotypical and male/female groups to increase coverage. We translate the seeds ourselves. As templates contain one more profession whose gender is unknown (here, NT: non-target), we experiment with either inflecting it to its feminine form (NT-Female), its masculine form (NT-Male), or a randomly choosing between the two (NT-Random). See Appendix D.1 for full details on the few-shot prompt construction.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"Moreover, our approach leads to significant improvement over random sampling (see Appendix D.2 for details on significance). Overall, these findings prove that interpretability scores, here apron,prof , can serve as a reliable signal to make fairer translations. We highlight how such improvements are enabled by a simple solution that requires no fine-tuning and only four human-written examples.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"In both stereotypical and nonstereotypical examples, we observe a correct shift in articles (“El” for male, “La” for female) and gender inflection corresponding to the profession (e.g., “the librarian” - “el bibliotecario” (male), “la bibliotecaria” (female)). Interestingly, while Flan-T5 translates poorly the profession “clerk” with “el secretario” (second row), Flan-T5Few-Shot chooses the right word and gender inflection (“la empleada”). We attribute this improvement in translation to the presence of the profession “clerk” in the few-shot examples, which likely allows the model to learn the correct profession translation.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"We also observe the behavior of the Flan-T5 and Flan-T5Few-Shot models across stereotypical and anti-stereotypical examples. Using the few-shot debiasing in Spanish, the model demonstrates a higher success rate in correcting the profession translations associated with anti-stereotypical examples (235) compared to stereotypical examples (80) out of a total of 348 identified examples.",0
A Tale of Pronouns Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation,"To broaden our study and provide groundwork on interpretability for gender-neutral MT, we conducted a preliminary analysis of the 240 WinoMT samples requiring gender-neutral translation. These instances result from compiling templates with a gender-neutral pronoun (e.g., “The technician told the customer that they could pay with cash.”). Table 10 provides a detailed overview of the results for Flan-T5 and mT0. Considering Flan-T5 and En-Es, we did not find either inflected form in 67 (28%) cases (i.e., they did not match with any entry in our dictionary).",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due to severe domain shifts. Most prior works deal with this problem in a two-pass pipeline manner based on metric learning. In practice, these dominant pipeline models may be limited in computational efficiency and generalization capacity because of non-parallel inference and context free discrete label embeddings.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"To this end, we re-examine the typical metric-based methods, and propose a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling. Considering simplicity, efficiency and generalizability, we present a cascade-style joint learning framework coupled with context aware soft label representations and slot-level contrastive representation learning to mitigate the data and label shift problems effectively. Extensive experiments on public benchmarks demonstrate the superiority of the proposed approach over a series of competitive baselines.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Slot filling, as an essential component widely exploited in task-oriented conversational systems, has attracted increasing attention recently (Zhang and Wang, 2016; Goo et al., 2018; Gangadharaiah and Narayanaswamy, 2019). It aims to identify a specific type (e.g., artist and playlist) for each slot entity from a given user utterance. Owing to the rapid development of deep neural networks and with help from large-scale annotated data, research on slot filling has made great progress with considerable performance improvement (Qin et al., 2019; Wu et al., 2020; Qin et al., 2020, 2021).",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Despite the remarkable accomplishments, there are at least two potential challenges in realistic application scenarios. First is the data scarcity problem in specific target domains (e.g., Healthcare and E-commerce). The manually-annotated training data in these domains is probably unavailable, and even the unlabeled training data might be hard to acquire (Jia et al., 2019; Liu et al., 2020a). As a result, the performance of slot filling models may drop significantly due to extreme data distribution shifts.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In this work, we revisit the metric-based zeroshot cross-domain slot filling under challenging domain (both data and label) shifts. We propose an adaptive end-to-end metric learning scheme to improve the efficiency and effectiveness of the zeroshot model in favor of practical applications. For one thing, we provide a cascade-style joint learning architecture well coupled with the slot boundary module and type matching module, allowing for knowledge sharing among the sub-modules and higher computational efficiency. Moreover, the soft label embeddings are adaptively learnt by capturing the correlation between slot labels and utterance.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Zero-shot domain generalization has been shown to be a feasible solution to bridge the gap of domain shifts with no access to data from the target domain. Recent dominating advances focus on the two-step pipeline fashion to learn the zero-shot model using the metric learning paradigms (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021; Siddique et al., 2021). Nevertheless, besides inefficient inference resulted from non-parallelization the generalization capability of these models may be limited due to lack of knowledge sharing between sub-modules, and context-independent discrete static label embeddings. Although the alternative question-answering (QA) based methods (Du et al., 2021; Yu et al., 2021; Liu et al., 2022) are able to achieve impressive results, they need to manually design and construct the questions/- queries, essentially introducing detailed descriptive information about the slot types.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"The second is the existence of label shifts (as shown in the example in Figure 1). The target domain may contain novel slot types unseen in the source-domain label space (Liu et al., 2018; Shah et al., 2019; Liu et al., 2020b; Wang et al., 2021), namely there is a mismatch between different domain label sets. This makes it difficult to apply the source models to completely unseen target domains that are unobservable during the training process.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Finally, to verify the effectiveness of the proposed method, we carry out extensive experiments on different benchmark datasets. The empirical studies show the superiority of our method, which achieves effective performance gains compared to several competitive baseline methods. Overall, the main contributions can be summarized as follows: (1) Compared with existing metric-based methods, we propose a more efficient and effective end-to-end scheme for zero-shot slot filling, and show our soft label embeddings perform much better than previous commonly-used static label representations. (2) We investigate the slot level contrastive learning to effectively improve generalization capacity for zero-shot slot filling. (3) By extensive experiments, we demonstrate the benefits of our model in comparison to the existing metric-based methods, and provide an insightful quantitative and qualitative analysis.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In order to deal with variable slot types within an unknown domain, we discard the standard sequence labeling paradigm by cross-labeling (e.g., B-playlist, I-playlist). Instead, we adopt a cascade-style architecture coupled with the slot boundary module and typing module under a joint learning framework. The boundary module is used to detect whether the tokens in an utterance are slot terms or not by the CRF-based labeling method with BIO schema, while the typing module is used to match the most likely type for the corresponding slot term using the metric-based method. Since pretraining model is beneficial to learn general representations, we adopt the pre-trained BERT (Devlin et al., 2019) as our backbone encoder.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Recent line of works have investigated the instance-level contrastive learning by template regularization (Shah et al., 2019; Liu et al., 2020b; He et al., 2020; Wang et al., 2021). As slots with the same types tend to have the semantically similar contexts, inspired by Das et al. (2022), we propose to use the slot-level contrastive learning to facilitate the discriminative slot representations that may contribute to adaptation robustness.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Although slot boundary module can select the slot terms from an utterance, it fails to learn discriminative slot entities. Thus, we design a typing module to achieve it in parallel by semantic similarity matching between slot labels and utterance tokens. Concretely, we take advantage of the above boundary information to locate the slot entity tokens of the utterance. We specially exploit the soft-weighting boundary embedding vectors for enabling differentiable joint training, which are combined with the contextual utterance representations to obtain the boundary-enhanced representations.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"For each slot type, the slot label matrix is obtained by averaging over the representations of the slot label tokens. Unlike the conventional discrete and static label embeddings (Liu et al., 2020b; Siddique et al., 2021; Ma et al., 2022) that capture the semantics of each textual label separately, we attempt to build the label-utterance correlation, and the adaptive interaction between the slot labels and utterance tokens encourages the model to learn the context-aware soft label embeddings dynamically, which will be exploited as the supervision information for the metric learning.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"To evaluate the proposed method, we conduct the experiments on the SNIPS dataset for zero-shot settings (Coucke et al., 2018), which contains 39 slot types across seven different domains: AddToPlaylist (ATP), BookRestaurant (BR), GetWeather (GW), PlayMusic (PM), RateBook (RB), SearchCreativeWork (SCW) and SearchScreeningEvent (SSE). Following previous studies (Liu et al., 2020b; Siddique et al., 2021), we choose one of these domains as the target domain never used for training, and the remaining six domains are combined to form the source domain. Then, we split 500 samples in the target domain as the development set and the remainder are used for the test set.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"We compare our method with the following competitive baselines using the pre-trained BERT as encoder: (1) Coach. Liu et al. (2020b) propose a two-step pipeline matching framework assisted by template regularization; (2) PCLC.Wang et al. (2021) propose a prototypical contrastive learning method with label confusion; (3) LEONA. Siddique et al. (2021) propose to integrate linguistic knowledge (e.g., external NER and POS-tagging cues) into the basic framework. Although not our focused baselines, we also compare against the advanced generative baselines (Li et al., 2023) with T5-Large and QA-based methods (Du et al., 2021; Liu et al., 2022) that require manual efforts to convert slot type descriptions into sentential queries/questions, and process by means of the machine reading comprehension (MRC) architecture (Li et al., 2020).",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"We use the pre-trained uncased BERTBASE model5 as the backbone encoder. The dimension of the boundary embedding is set to 10. We use 0.1 dropout ratio for slot filling and 0.5 for NER. For the contrastive learning module, we use the cosine metric function and select the optimal temperature τ from 0.1 to 1. During training, the AdamW (Loshchilov and Hutter, 2019) optimizer with a mini-batch size 32 is applied to update all trainable parameters, and the initial learning rate is set to 2e-5 for BERT and 1e-3 for other modules. All the models are trained on NIVIDIA GeForce RTX 3090Ti GPUs for up to 30 epochs. The averaged F1-score over five runs is used to evaluate the performance. The best-performing model on the development set is used for testing.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"As shown in Table 1, our method achieves more promising performance than previously proposed metric-based methods on various target domains, with an average about 5% improvements compared with the strong baseline LEONA. We attribute it to the fact that our proposed joint learning model make full use of the sub-modules, and the context-aware soft label embeddings provide better prototype representations. Moreover, we also observe that the slot-level contrastive learning plays an important role in improving adaptation performance.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Our model with Slot-CL obtains consistent performance gains over almost all the target domains except for the SSE domain. We suspect that it may result from slot entity confusion. For example, for slot entities “cinema” and “theatre” from SSE, they are usually annotated with object_location_type, but “cinemas” in “caribbean cinemas” and “theatres” in “star theatres” are annotated with location_name, which is prone to be misled by the contrastive objective. Additionally, without introducing extra manual prior knowledge, our method achieves very competitive performance compared with the QA-based baselines.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In order to better understand our method, we further present some quantitative and qualitative analyses that provides some insights into why our method works and where future work could potentially improve it. One advantage of our framework is the efficient inference process benefiting from the well-parallelized design. We evaluate the speed by running the model one epoch on the BookRestaurant test data with batch size set to 32. Results in Table 3 show that our method achieves ×13.89 and ×7.06 speedup compared with the advanced metric-based method (i.e., LEONA) and QA-based method (i.e., SLMRC). This could be attributed to our batch-wise decoding in parallel.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Here we examine how our model benefits from the label-utterance interaction. As presented in Table 4, the performance of our model drops significantly when eliminating the interaction from different aspects , justifying our design. Compared to the other degraded interaction strategies, the utterance-label interaction helps learn the context-aware label embeddings, namely the utterance provides the context cues for the slot labels. Furthermore, we also notice that interaction between slot labels also makes sense. When only let each slot label attend to itself and the utterance, we observe the performance drop probably due to the loss of discriminative information among different slot labels.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Here we explore several typical distance metric functions (including Cosine, MSE, Smooth L1, and KLdivergence) for the slot-level contrastive objective, and we also consider the influence of temperature τ . Figure 4 reveals that the temperature value directly affects the final performance. Also, it shows better results overall at around τ = 0.5 for each metric function we take. We select the cosine similarity function as our desired distance metric function, due to its relatively good performance.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"We study the effect of different types of label embeddings. Figure 3 shows the comparison results. We can see that the proposed context-aware soft label embedding outperforms other purely discrete or decoupled embeddings, including discrete BERT, decoupled BERT or GloVe (Pennington et al., 2014) embeddings. Interestingly, when fine-tuning, we find that BERTdis works slightly better than BERTdec, as it might be harmful to tune soft label embeddings without utterance contexts. Furthermore, we observe a significant improvement of our model when incorporating the GloVe static vectors, suggesting that richer label semantics can make a positive difference. Meanwhile, the discrete or decoupled label embeddings without fine-tuning may yield better results.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"To verify the effectiveness of our method in the few-shot setting where the target domain has a small amount of training examples, we conduct experiments in the 20-shot and 50-shot scenarios. In line with previous works, we take the first K examples in the development set for training named the K-Shot scenario and the remaining keeps for evaluation. Table 5 illustrates that our method achieves superior performance compared with other representative metric-based methods. However, we also notice that our method without the slot-level contrastive learning obtains limited absolute improvements as data size increase, indicating the slot-level contrastive learning performs better in this case.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Since label shift is a critical challenge in zero-shot learning, to verify the generalization capacity, we specifically test our method on the unseen target data. Following Liu et al. (2022), we split the dataset into the seen and unseen group, where we only evaluate on unseen slot entities during training in the unseen slot group, while evaluate on the whole utterance in the unseen uttr group. From Table 6, our method performs better than other metric-based baselines, showing the superiority of our method for unseen domain generalization.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Considering slot labels and utterances may vary significantly across different datasets, we further evaluate the proposed method under the cross-dataset scenario, a more challenging setting. Here we introduce another popular slot filling dataset ATIS (Liu et al., 2019a). It is used for the target (source) domain data while the SNIPS for the source (target) domain data, as shown in Table 7. The results confirm that our method still works well in this challenging setting.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In recent years, zero-shot slot filling has received increasing attention. A dominating line of research is the metric-learning method, where the core idea is to learn a prototype representation for each category and classify test data based on their similarities with prototypes (Snell et al., 2017). For slot filling, the semantic embeddings of textual slot descriptions usually serve as the prototype representations (Bapna et al., 2017; Lee and Jha, 2019; Zhu et al., 2020). Shah et al. (2019) utilize both the slot description and a few examples of slot values to learn semantic representations of slots.  ",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Furthermore, various two-pass pipeline schemes are proposed by separating the slot filling task into two steps along with template regularization (Liu et al., 2020b), adversarial training (He et al., 2020), contrastive learning (Wang et al., 2021), linguistic prior knowledge (Siddique et al., 2021). However, these mostly utilize the context-free discrete label embeddings, and the two-pass fashion has potential limitations due to a lack of knowledge sharing between sub-modules as well as inefficient inference. These motivate us to exploit the context-aware label representations under an end-to-end joint learning framework. Another line of research is the QA-based methods that borrow from question-answering systems, relying on manually well-designed queries.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Du et al. (2021) use a set of slot-to-question generation strategies and pre-train on numerous synthetic QA pairs. Yu et al. (2021) and Liu et al. (2022) apply the MRC framework (Li et al., 2020) to overcome the domain shift problem. Heo et al. (2022) modify the MRC framework into sequence-labeling style by using each slot label as query. Li et al. (2023) introduce a generative framework using each slot label as prompt. In our work, we mainly focus on the metric-based method without intentionally introducing external knowledge with manual efforts.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"The key idea is to learn discriminative feature representations by contrasting positive pairs against negative pairs. Namely, those with similar semantic meanings are pushed towards each other in the embedding space while those with different semantic meanings are pulled apart each other. Yan et al. (2021) and Gao et al. (2021) explore instance-level self-supervised contrastive learning where sample pairs are constructed by data augmentation. Khosla et al. (2020) further explore the supervised setting by contrasting the set of all instances from the same class against those from the other classes. Das et al. (2022) present a token-level supervised contrastive learning solution to deal with the few-shot NER task by means of Gaussian embeddings.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Previous studies for slot filling mainly focus on instance-level contrastive learning, which may be sub-optimal for a fine-grained sequence labeling task. Inspired by supervised contrastive learning, we leverage a slot-level contrastive learning scheme for zero-shot slot filling to learn the discriminative representations for domain adaptation. For all existing slot entities within a mini-batch, we regard those with the same type as the positive example pairs and those with different type as negative ones.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"In this paper, we tackle the problem of generalized zero-shot slot filling by the proposed end-toend metric learning based scheme. We propose a cascade-style multi-task learning framework to efficiently detect the slot entity from a target domain utterance. The context-aware soft label embeddings are shown to be superior to the widely-used discrete ones. Regarding domain adaptation robustness, we propose a slot level contrastive learning scheme to facilitate the discriminative representations of slot entities.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Extensive experiments across various domain datasets demonstrate the effectiveness of the proposed approach when handling unseen target domains. Our investigation also confirms that semantically richer label representations enable help further boost the recognition performance, which motivates us to further explore external knowledge enhanced soft label embeddings for advancing the metric-based method.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"Although our work makes a further progress in the challenging zero-shot slot filling, it is subject to several potential limitations. Firstly, since slot label sequence is used as the prefix of the utterance, this directly results in a long input sequence. Secondly, our method may be negatively affected by severe label ambiguity. There are some slot entities with rather similar semantics, leading to wrong slot type predictions. For example, “book a manadonese restaurant”, the slot entity type of “manadonese” is actually cuisine, but is easily identified as country.",0
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling,"One major reason is that some utterances are relatively short and lack sufficient contextual cues. Thirdly, the recognition performance of metric-based methods may remain difficult to exceed that of advanced QA-based or generative methods due to the fact that the latter manually introduces detailed slot label description by well-designed queries or prompts.",0
